# knowledge-distillation-of-large-language-models


It is common to use advanced LLMs to generate high-quality synthetic data to fine tune smaller language models. Also, recently there has been a lot of interest in using a strong LLM as judge to evaluate performace of a weaker model. We ask: 

When we have access to human annotated high quality data, is there any advantage to having access to  high-quality synthetic data of a stronger LLM for training of a weaker model?

<center>
<img alt="fig1" width="800px" src="LLM aligned SFT.png">
</center>
